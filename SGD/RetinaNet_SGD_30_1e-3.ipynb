{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gR3njCUJt5Aq"
   },
   "source": [
    "# **Install Dependencies Detecron2 & Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31156,
     "status": "ok",
     "timestamp": 1750772471074,
     "user": {
      "displayName": "Al Qunnah",
      "userId": "06050257527007666949"
     },
     "user_tz": -420
    },
    "id": "wSlhVvLn9l-x",
    "outputId": "a297d982-fe12-421b-d85c-67003f76acea"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9547,
     "status": "ok",
     "timestamp": 1749873241573,
     "user": {
      "displayName": "Al Qunnah",
      "userId": "06050257527007666949"
     },
     "user_tz": -420
    },
    "id": "tKVogFBYkHwu",
    "outputId": "95aa4552-f2a0-4b36-c092-56826d378d5d"
   },
   "outputs": [],
   "source": [
    "# üóÇ Buat direktori target\n",
    "!mkdir -p \"/content/drive/MyDrive/210411100014_Taufiqu Reza Yoga Pratama/Dataset/dataset_new\"\n",
    "\n",
    "# ‚¨áÔ∏è Unduh file zip dari Roboflow ke direktori target\n",
    "!curl -L \"https://app.roboflow.com/ds/g32luldSN8?key=kVAIz6q98E\" -o \"/content/drive/MyDrive/210411100014_Taufiqu Reza Yoga Pratama/Dataset/dataset_new/roboflow.zip\"\n",
    "\n",
    "# üì¶ Ekstrak isi zip ke folder yang sama\n",
    "!unzip -q \"/content/drive/MyDrive/210411100014_Taufiqu Reza Yoga Pratama/Dataset/dataset_new/roboflow.zip\" -d \"/content/drive/MyDrive/210411100014_Taufiqu Reza Yoga Pratama/Dataset/dataset_new\"\n",
    "\n",
    "# üßπ Hapus file zip setelah diekstrak\n",
    "!rm \"/content/drive/MyDrive/210411100014_Taufiqu Reza Yoga Pratama/Dataset/dataset_new/roboflow.zip\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 245968,
     "status": "ok",
     "timestamp": 1750772719914,
     "user": {
      "displayName": "Al Qunnah",
      "userId": "06050257527007666949"
     },
     "user_tz": -420
    },
    "id": "QQzKtbN-grjv",
    "outputId": "769f27c7-cb61-440c-e407-3954379f3433"
   },
   "outputs": [],
   "source": [
    "# üì¶ Install dependencies\n",
    "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
    "!pip install -q pycocotools\n",
    "\n",
    "# üîß Setup: Import & Register Dataset\n",
    "import os\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "import random\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Adjust these if your paths differ after unzip\n",
    "# dataset_name = \"roboflow\"\n",
    "# img_dir = \"train\"\n",
    "# train_json = \"train/_annotations.coco.json\"\n",
    "# val_json = \"valid/_annotations.coco.json\"\n",
    "\n",
    "# register_coco_instances(\"roboflow_train\", {}, train_json, img_dir)\n",
    "# register_coco_instances(\"roboflow_val\", {}, val_json, \"valid\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3aN_oUqxzZFS"
   },
   "source": [
    "# **Register Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5139,
     "status": "ok",
     "timestamp": 1750736421657,
     "user": {
      "displayName": "Al Qunnah",
      "userId": "06050257527007666949"
     },
     "user_tz": -420
    },
    "id": "TgAQg8_AU6gP",
    "outputId": "bfddb246-20b9-4a40-b363-c67f803c316f"
   },
   "outputs": [],
   "source": [
    "# Set root folder di Google Drive\n",
    "dataset_root = \"/content/drive/MyDrive/210411100014_Taufiqu Reza Yoga Pratama/Dataset/dataset_new\"\n",
    "\n",
    "# Buat path lengkap ke JSON dan folder gambar\n",
    "train_json = os.path.join(dataset_root, \"train/_annotations.coco.json\")\n",
    "val_json = os.path.join(dataset_root, \"valid/_annotations.coco.json\")\n",
    "test_json = os.path.join(dataset_root, \"test/_annotations.coco.json\")\n",
    "train_img_dir = os.path.join(dataset_root, \"train\")\n",
    "val_img_dir = os.path.join(dataset_root, \"valid\")\n",
    "test_img_dir = os.path.join(dataset_root, \"test\")\n",
    "\n",
    "# ‚úÖ Cek apakah file dan folder ada\n",
    "print(\"Train JSON exists:\", os.path.exists(train_json))\n",
    "print(\"Train Image Dir exists:\", os.path.exists(train_img_dir))\n",
    "\n",
    "# üóÇ Daftarkan dataset ke Detectron2\n",
    "register_coco_instances(\"roboflow_train\", {}, train_json, train_img_dir)\n",
    "register_coco_instances(\"roboflow_val\", {}, val_json, val_img_dir)\n",
    "register_coco_instances(\"roboflow_test\", {}, test_json, test_img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 979,
     "status": "ok",
     "timestamp": 1749969918304,
     "user": {
      "displayName": "Al Qunnah",
      "userId": "06050257527007666949"
     },
     "user_tz": -420
    },
    "id": "CBgUh2drVF5s",
    "outputId": "e7f2c6ad-b19e-49fd-829c-fa50dc55131e"
   },
   "outputs": [],
   "source": [
    "# Cek sample data target\n",
    "dataset_dicts = DatasetCatalog.get(\"roboflow_train\")\n",
    "print(dataset_dicts[0])  # satu contoh anotasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 768,
     "status": "ok",
     "timestamp": 1749969984180,
     "user": {
      "displayName": "Al Qunnah",
      "userId": "06050257527007666949"
     },
     "user_tz": -420
    },
    "id": "B9h5OtzjVsxg",
    "outputId": "e1e7af0f-458b-4468-c889-9f3b6f4cadb7"
   },
   "outputs": [],
   "source": [
    "for i, data in enumerate(dataset_dicts):\n",
    "    print(f\"\\n===== Data {i} =====\")\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1392,
     "status": "ok",
     "timestamp": 1750476226394,
     "user": {
      "displayName": "Al Qunnah",
      "userId": "06050257527007666949"
     },
     "user_tz": -420
    },
    "id": "fzfA2ozwdGci",
    "outputId": "db8dee00-b3a6-4c34-a450-dd81f2f24c3e"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Path ke file JSON kamu\n",
    "json_path = \"/content/drive/MyDrive/210411100014_Taufiqu Reza Yoga Pratama/Dataset/_annotations_fixed.coco.json\"\n",
    "\n",
    "# Buka dan baca file JSON\n",
    "with open(json_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Tampilkan isi kategori\n",
    "print(\"== Kategori dalam file COCO JSON ==\")\n",
    "for cat in data['categories']:\n",
    "    print(f\"  ID: {cat['id']}  |  Name: {cat['name']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 645,
     "status": "ok",
     "timestamp": 1750240901071,
     "user": {
      "displayName": "Al Qunnah",
      "userId": "06050257527007666949"
     },
     "user_tz": -420
    },
    "id": "_4clXkjvevvl",
    "outputId": "9481c89b-e549-4c3e-a8cf-4ec61b285a28"
   },
   "outputs": [],
   "source": [
    "# Cek sample data target\n",
    "dataset_dicts_1 = DatasetCatalog.get(\"roboflow_val\")\n",
    "print(dataset_dicts_1[0])  # satu contoh anotasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 433,
     "status": "ok",
     "timestamp": 1750240923007,
     "user": {
      "displayName": "Al Qunnah",
      "userId": "06050257527007666949"
     },
     "user_tz": -420
    },
    "id": "7koezuc0fNzN",
    "outputId": "d70b7cdd-ff88-4f0f-9ec7-d776533c539f"
   },
   "outputs": [],
   "source": [
    "for i, data in enumerate(dataset_dicts_1):\n",
    "    print(f\"\\n===== Data {i} =====\")\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 633,
     "status": "ok",
     "timestamp": 1750240904550,
     "user": {
      "displayName": "Al Qunnah",
      "userId": "06050257527007666949"
     },
     "user_tz": -420
    },
    "id": "vFZip6B5ewkM",
    "outputId": "059553a4-c9e8-430e-e9e0-983a6ecde8cf"
   },
   "outputs": [],
   "source": [
    "# Cek sample data target\n",
    "dataset_dicts_2 = DatasetCatalog.get(\"roboflow_test\")\n",
    "print(dataset_dicts_2[0])  # satu contoh anotasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 185,
     "status": "ok",
     "timestamp": 1750240951558,
     "user": {
      "displayName": "Al Qunnah",
      "userId": "06050257527007666949"
     },
     "user_tz": -420
    },
    "id": "-1P-JCjbfWc7",
    "outputId": "c1ee901d-49b7-4aeb-d118-683e6b2720b5"
   },
   "outputs": [],
   "source": [
    "for i, data in enumerate(dataset_dicts_2):\n",
    "    print(f\"\\n===== Data {i} =====\")\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lepBBmENzeVV"
   },
   "source": [
    "# **Konfigurasi Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4sJRoQjDXqVO"
   },
   "source": [
    "**Penghitungan Epoch = jumlah data train//batch_size = 892//16 = 56**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3262,
     "status": "ok",
     "timestamp": 1750772795714,
     "user": {
      "displayName": "Al Qunnah",
      "userId": "06050257527007666949"
     },
     "user_tz": -420
    },
    "id": "0mjZzpbxgu81",
    "outputId": "2bd026ee-c0b6-45f7-8e33-0f92e5043999"
   },
   "outputs": [],
   "source": [
    "from detectron2 import model_zoo\n",
    "\n",
    "# üß† Configure RetinaNet\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/retinanet_R_50_FPN_3x.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (\"roboflow_train\",)\n",
    "cfg.DATASETS.TEST = (\"roboflow_val\",)\n",
    "cfg.DATALOADER.NUM_WORKERS = 4\n",
    "cfg.MODEL.WEIGHTS = \"detectron2://ImageNetPretrained/MSRA/R-50.pkl\"\n",
    "cfg.SOLVER.IMS_PER_BATCH = 16\n",
    "cfg.SOLVER.BASE_LR = 0.001\n",
    "cfg.SOLVER.MAX_ITER = 1680\n",
    "cfg.TEST.EVAL_PERIOD = 500  # Evaluate every 500 iterations\n",
    "cfg.SOLVER.STEPS = []\n",
    "cfg.MODEL.RETINANET.NUM_CLASSES = 4\n",
    "cfg.OUTPUT_DIR = \"/content/drive/MyDrive/210411100014_Taufiqu Reza Yoga Pratama/Model/new/epoch_30_1e-3_SGD\"\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# üì¶ Tambahan: Tampilkan konfigurasi anchor\n",
    "print(\"üìê Anchor Sizes per FPN level (cfg.MODEL.ANCHOR_GENERATOR.SIZES):\")\n",
    "for i, sizes in enumerate(cfg.MODEL.ANCHOR_GENERATOR.SIZES):\n",
    "    print(f\"  Level P{i+3}: Scales = {sizes}\")\n",
    "\n",
    "print(\"\\nüî¢ Aspect Ratios (cfg.MODEL.ANCHOR_GENERATOR.ASPECT_RATIOS):\")\n",
    "print(f\"  {cfg.MODEL.ANCHOR_GENERATOR.ASPECT_RATIOS}\")\n",
    "\n",
    "# üîÅ Hitung jumlah anchor per lokasi\n",
    "num_scales = len(cfg.MODEL.ANCHOR_GENERATOR.SIZES[0])  # asumsi semua level punya scale yang sama\n",
    "num_ratios = len(cfg.MODEL.ANCHOR_GENERATOR.ASPECT_RATIOS[0])\n",
    "num_anchors_per_location = num_scales * num_ratios\n",
    "print(f\"\\nüì¶ Jumlah anchor per lokasi (k): {num_anchors_per_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rdy93Ji1tf_r"
   },
   "source": [
    "# **Data Target**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1gy0_1Ntk9f"
   },
   "source": [
    "**Percobaan Dummy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 899,
     "status": "ok",
     "timestamp": 1750478194185,
     "user": {
      "displayName": "Al Qunnah",
      "userId": "06050257527007666949"
     },
     "user_tz": -420
    },
    "id": "YiuxsK7Qld7n",
    "outputId": "e3c8e5c8-0f2c-4dff-c714-7cb6f260d61d"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2 import model_zoo\n",
    "\n",
    "# === 1. Setup config\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/retinanet_R_50_FPN_1x.yaml\"))\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/retinanet_R_50_FPN_1x.yaml\")\n",
    "cfg.MODEL.RETINANET.NUM_CLASSES = 3\n",
    "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# === 2. Build model\n",
    "model = build_model(cfg)\n",
    "model.eval()\n",
    "DetectionCheckpointer(model).load(cfg.MODEL.WEIGHTS)\n",
    "\n",
    "# === 3. Dummy input image\n",
    "dummy_input = torch.randn(1, 3, 512, 512).to(cfg.MODEL.DEVICE)\n",
    "\n",
    "# === 4. Dapatkan fitur dan output head\n",
    "with torch.no_grad():\n",
    "    features = model.backbone(dummy_input)\n",
    "    feature_list = [features[f'p{l}'] for l in range(3, 8)]\n",
    "    head_outputs = model.head(feature_list)\n",
    "\n",
    "# === 5. Hitung anchor box per level\n",
    "anchor_shapes = {}\n",
    "classification_shapes = {}\n",
    "regression_shapes = {}\n",
    "\n",
    "# Perbaikan penting di sini\n",
    "num_scales = len(cfg.MODEL.ANCHOR_GENERATOR.SIZES[0])        # biasanya 3\n",
    "num_ratios = len(cfg.MODEL.ANCHOR_GENERATOR.ASPECT_RATIOS[0])  # biasanya 3\n",
    "num_anchors_per_location = num_scales * num_ratios            # = 9\n",
    "\n",
    "num_classes = cfg.MODEL.RETINANET.NUM_CLASSES\n",
    "\n",
    "for fpn_level in range(3, 8):\n",
    "    feature = features[f'p{fpn_level}']\n",
    "    N, _, H, W = feature.shape\n",
    "\n",
    "    cls_shape = (N, num_anchors_per_location * num_classes, H, W)\n",
    "    reg_shape = (N, num_anchors_per_location * 4, H, W)\n",
    "\n",
    "    classification_shapes[f'p{fpn_level}'] = cls_shape\n",
    "    regression_shapes[f'p{fpn_level}'] = reg_shape\n",
    "\n",
    "    total_anchors = H * W * num_anchors_per_location\n",
    "    anchor_shapes[f'p{fpn_level}'] = total_anchors\n",
    "\n",
    "# === 6. Print hasil\n",
    "total_anchor_count = sum(anchor_shapes.values())\n",
    "\n",
    "print(\"Jumlah Anchor per FPN level:\")\n",
    "for level, count in anchor_shapes.items():\n",
    "    print(f\"{level}: {count} anchor\")\n",
    "\n",
    "print(\"\\nOutput Shape Klasifikasi:\")\n",
    "for level, shape in classification_shapes.items():\n",
    "    print(f\"{level}: {shape}\")\n",
    "\n",
    "print(\"\\nOutput Shape Regresi:\")\n",
    "for level, shape in regression_shapes.items():\n",
    "    print(f\"{level}: {shape}\")\n",
    "\n",
    "print(f\"\\nTotal Anchor: {total_anchor_count}\")\n",
    "print(f\"Klasifikasi: ({total_anchor_count}, {cfg.MODEL.RETINANET.NUM_CLASSES})\")\n",
    "print(f\"Regresi: ({total_anchor_count}, 4)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RN7mEWK3tqdB"
   },
   "source": [
    "**Pada Data Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7713,
     "status": "ok",
     "timestamp": 1750736687487,
     "user": {
      "displayName": "Al Qunnah",
      "userId": "06050257527007666949"
     },
     "user_tz": -420
    },
    "id": "8zgQQPX9pWgf",
    "outputId": "cf347860-8c5b-4d75-e17e-75651103034a"
   },
   "outputs": [],
   "source": [
    "# === 1. Import library ===\n",
    "import torch\n",
    "import os\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2.data import DatasetCatalog, detection_utils as utils\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "\n",
    "# === 2. Path dataset ===\n",
    "annotation_path = \"/content/drive/MyDrive/210411100014_Taufiqu Reza Yoga Pratama/Dataset/dataset_new/train/_annotations.coco.json\"\n",
    "image_dir = \"/content/drive/MyDrive/210411100014_Taufiqu Reza Yoga Pratama/Dataset/dataset_new/train\"\n",
    "\n",
    "if not os.path.exists(annotation_path):\n",
    "    raise FileNotFoundError(f\"File anotasi tidak ditemukan: {annotation_path}\")\n",
    "if not os.path.exists(image_dir):\n",
    "    raise FileNotFoundError(f\"Folder gambar tidak ditemukan: {image_dir}\")\n",
    "\n",
    "# === 3. Registrasi dataset COCO ===\n",
    "register_coco_instances(\"my_train_2\", {}, annotation_path, image_dir)\n",
    "\n",
    "# === 4. Konfigurasi model Detectron2 ===\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/retinanet_R_50_FPN_1x.yaml\"))\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/retinanet_R_50_FPN_1x.yaml\")\n",
    "cfg.MODEL.RETINANET.NUM_CLASSES = 3\n",
    "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "cfg.DATASETS.TRAIN = (\"my_train_2\",)\n",
    "cfg.DATASETS.TEST = (\"my_train_2\",)\n",
    "\n",
    "# Atur resolusi input tetap\n",
    "cfg.INPUT.MIN_SIZE_TEST = 512\n",
    "cfg.INPUT.MAX_SIZE_TEST = 512\n",
    "cfg.INPUT.MIN_SIZE_TRAIN = 512\n",
    "cfg.INPUT.MAX_SIZE_TRAIN = 512\n",
    "cfg.INPUT.MIN_SIZE_TRAIN_SAMPLING = \"choice\"\n",
    "\n",
    "# === 5. Bangun model dan load bobot ===\n",
    "model = build_model(cfg)\n",
    "model.eval()\n",
    "DetectionCheckpointer(model).load(cfg.MODEL.WEIGHTS)\n",
    "\n",
    "# === 6. Ambil data dari dataset berdasarkan nama file ===\n",
    "dataset_dicts = DatasetCatalog.get(\"my_train_2\")\n",
    "\n",
    "# === Ganti nama file sesuai yang kamu inginkan ===\n",
    "target_filename = \"00000732_005_jpg.rf.331df048715d39346021fe3e09b95fdc.jpg\"  # <-- Ganti ini sesuai nama file yang kamu cari\n",
    "\n",
    "data_dict = None\n",
    "for d in dataset_dicts:\n",
    "    if os.path.basename(d[\"file_name\"]) == target_filename:\n",
    "        data_dict = d\n",
    "        break\n",
    "\n",
    "if data_dict is None:\n",
    "    raise FileNotFoundError(f\"Gambar dengan nama '{target_filename}' tidak ditemukan dalam dataset.\")\n",
    "\n",
    "# === 7. Baca dan ubah gambar jadi input model ===\n",
    "# Baca gambar & konversi ke tensor dengan copy() untuk hindari stride negatif\n",
    "image = utils.read_image(data_dict[\"file_name\"], format=\"BGR\")\n",
    "image_tensor = torch.tensor(image.transpose(2, 0, 1).copy()).float().div(255.0).unsqueeze(0).to(cfg.MODEL.DEVICE)\n",
    "\n",
    "# === 8. Proses backbone dan head model ===\n",
    "with torch.no_grad():\n",
    "    features = model.backbone(image_tensor)\n",
    "    feature_list = [features[f'p{l}'] for l in range(3, 8)]\n",
    "    head_outputs = model.head(feature_list)\n",
    "\n",
    "# === 9. Hitung anchor dan bentuk output ===\n",
    "anchor_shapes = {}\n",
    "classification_shapes = {}\n",
    "regression_shapes = {}\n",
    "\n",
    "num_scales = len(cfg.MODEL.ANCHOR_GENERATOR.SIZES[0])        # default: 3\n",
    "num_ratios = len(cfg.MODEL.ANCHOR_GENERATOR.ASPECT_RATIOS[0])  # default: 3\n",
    "num_anchors_per_location = num_scales * num_ratios            # 9\n",
    "num_classes = cfg.MODEL.RETINANET.NUM_CLASSES\n",
    "\n",
    "for fpn_level in range(3, 8):\n",
    "    feature = features[f'p{fpn_level}']\n",
    "    N, _, H, W = feature.shape\n",
    "\n",
    "    cls_shape = (N, num_anchors_per_location * num_classes, H, W)\n",
    "    reg_shape = (N, num_anchors_per_location * 4, H, W)\n",
    "\n",
    "    classification_shapes[f'p{fpn_level}'] = cls_shape\n",
    "    regression_shapes[f'p{fpn_level}'] = reg_shape\n",
    "\n",
    "    total_anchors = H * W * num_anchors_per_location\n",
    "    anchor_shapes[f'p{fpn_level}'] = total_anchors\n",
    "\n",
    "# === 10. Tampilkan hasil ===\n",
    "total_anchor_count = sum(anchor_shapes.values())\n",
    "\n",
    "print(\"Jumlah Anchor per FPN level:\")\n",
    "for level, count in anchor_shapes.items():\n",
    "    print(f\"{level}: {count} anchor\")\n",
    "\n",
    "print(\"\\nOutput Shape Klasifikasi:\")\n",
    "for level, shape in classification_shapes.items():\n",
    "    print(f\"{level}: {shape}\")\n",
    "\n",
    "print(\"\\nOutput Shape Regresi:\")\n",
    "for level, shape in regression_shapes.items():\n",
    "    print(f\"{level}: {shape}\")\n",
    "\n",
    "print(f\"\\nTotal Anchor: {total_anchor_count}\")\n",
    "print(f\"Klasifikasi: ({total_anchor_count}, {num_classes})\")\n",
    "print(f\"Regresi: ({total_anchor_count}, 4)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 734,
     "status": "ok",
     "timestamp": 1750773135722,
     "user": {
      "displayName": "Al Qunnah",
      "userId": "06050257527007666949"
     },
     "user_tz": -420
    },
    "id": "eb95CZdXaqwN",
    "outputId": "2b68e16c-83a7-448d-c6eb-5965dfde857a"
   },
   "outputs": [],
   "source": [
    "# === 1. Import library ===\n",
    "import torch\n",
    "import os\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2.data import DatasetCatalog, detection_utils as utils\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.modeling.matcher import Matcher\n",
    "from detectron2.modeling.anchor_generator import DefaultAnchorGenerator\n",
    "from detectron2.structures import Boxes\n",
    "from detectron2.modeling.box_regression import Box2BoxTransform\n",
    "from detectron2.structures.boxes import pairwise_iou\n",
    "import numpy as np\n",
    "from detectron2.layers import ShapeSpec\n",
    "\n",
    "# === 2. Path dataset ===\n",
    "annotation_path = \"/content/drive/MyDrive/210411100014_Taufiqu Reza Yoga Pratama/Dataset/dataset_new/train/_annotations.coco.json\"\n",
    "image_dir = \"/content/drive/MyDrive/210411100014_Taufiqu Reza Yoga Pratama/Dataset/dataset_new/train\"\n",
    "\n",
    "if not os.path.exists(annotation_path):\n",
    "    raise FileNotFoundError(f\"File anotasi tidak ditemukan: {annotation_path}\")\n",
    "if not os.path.exists(image_dir):\n",
    "    raise FileNotFoundError(f\"Folder gambar tidak ditemukan: {image_dir}\")\n",
    "\n",
    "# === 3. Registrasi dataset COCO ===\n",
    "register_coco_instances(\"my_train_2\", {}, annotation_path, image_dir)\n",
    "\n",
    "# === 4. Konfigurasi model Detectron2 ===\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/retinanet_R_50_FPN_3x.yaml\"))\n",
    "\n",
    "# ‚úÖ Perbaikan Anchor Generator\n",
    "cfg.MODEL.ANCHOR_GENERATOR.SIZES = [\n",
    "    [32, 40.31747359663594, 50.79683366298238],\n",
    "    [64, 80.63494719327188, 101.59366732596476],\n",
    "    [128, 161.26989438654377, 203.18733465192952],\n",
    "    [256, 322.53978877308754, 406.37466930385904],\n",
    "    [512, 645.0795775461751, 812.7493386077181]\n",
    "]\n",
    "cfg.MODEL.ANCHOR_GENERATOR.ASPECT_RATIOS = [[0.5, 1.0, 2.0]]\n",
    "\n",
    "cfg.MODEL.WEIGHTS = \"detectron2://ImageNetPretrained/MSRA/R-50.pkl\"\n",
    "cfg.MODEL.RETINANET.NUM_CLASSES = 3\n",
    "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "cfg.DATASETS.TRAIN = (\"my_train_2\",)\n",
    "cfg.DATASETS.TEST = (\"my_train_2\",)\n",
    "\n",
    "cfg.INPUT.MIN_SIZE_TEST = 512\n",
    "cfg.INPUT.MAX_SIZE_TEST = 512\n",
    "cfg.INPUT.MIN_SIZE_TRAIN = 512\n",
    "cfg.INPUT.MAX_SIZE_TRAIN = 512\n",
    "cfg.INPUT.MIN_SIZE_TRAIN_SAMPLING = \"choice\"\n",
    "\n",
    "# === 5. Bangun model dan load bobot ===\n",
    "model = build_model(cfg)\n",
    "model.eval()\n",
    "DetectionCheckpointer(model).load(cfg.MODEL.WEIGHTS)\n",
    "\n",
    "# === 6. Ambil data dari dataset berdasarkan nama file ===\n",
    "dataset_dicts = DatasetCatalog.get(\"my_train_2\")\n",
    "\n",
    "# === Ganti nama file sesuai yang kamu inginkan ===\n",
    "target_filename = \"01a3c3d994d85ce5634d2d13c03fd4b0_jpg.rf.3e9209e148bae99e32c3050e2e46042b.jpg\"\n",
    "\n",
    "data_dict = None\n",
    "for d in dataset_dicts:\n",
    "    if os.path.basename(d[\"file_name\"]) == target_filename:\n",
    "        data_dict = d\n",
    "        break\n",
    "\n",
    "if data_dict is None:\n",
    "    raise FileNotFoundError(f\"Gambar dengan nama '{target_filename}' tidak ditemukan dalam dataset.\")\n",
    "\n",
    "# === 7. Baca dan ubah gambar jadi input model ===\n",
    "image = utils.read_image(data_dict[\"file_name\"], format=\"BGR\")\n",
    "image_tensor = torch.tensor(image.transpose(2, 0, 1).copy()).float().div(255.0).unsqueeze(0).to(cfg.MODEL.DEVICE)\n",
    "\n",
    "# === 8. Proses backbone dan head model ===\n",
    "with torch.no_grad():\n",
    "    features = model.backbone(image_tensor)\n",
    "    feature_list = [features[f'p{l}'] for l in range(3, 8)]\n",
    "    head_outputs = model.head(feature_list)\n",
    "\n",
    "# === 9. Hitung anchor dan bentuk output ===\n",
    "anchor_shapes = {}\n",
    "classification_shapes = {}\n",
    "regression_shapes = {}\n",
    "\n",
    "num_scales = len(cfg.MODEL.ANCHOR_GENERATOR.SIZES[0])        # default: 3\n",
    "num_ratios = len(cfg.MODEL.ANCHOR_GENERATOR.ASPECT_RATIOS[0])  # default: 3\n",
    "num_anchors_per_location = num_scales * num_ratios            # 9\n",
    "num_classes = cfg.MODEL.RETINANET.NUM_CLASSES\n",
    "\n",
    "for fpn_level in range(3, 8):\n",
    "    feature = features[f'p{fpn_level}']\n",
    "    N, _, H, W = feature.shape\n",
    "\n",
    "    cls_shape = (N, num_anchors_per_location * num_classes, H, W)\n",
    "    reg_shape = (N, num_anchors_per_location * 4, H, W)\n",
    "\n",
    "    classification_shapes[f'p{fpn_level}'] = cls_shape\n",
    "    regression_shapes[f'p{fpn_level}'] = reg_shape\n",
    "\n",
    "    total_anchors = H * W * num_anchors_per_location\n",
    "    anchor_shapes[f'p{fpn_level}'] = total_anchors\n",
    "\n",
    "total_anchor_count = sum(anchor_shapes.values())\n",
    "\n",
    "print(\"Jumlah Anchor per FPN level:\")\n",
    "for level, count in anchor_shapes.items():\n",
    "    print(f\"{level}: {count} anchor\")\n",
    "\n",
    "print(\"\\nOutput Shape Klasifikasi:\")\n",
    "for level, shape in classification_shapes.items():\n",
    "    print(f\"{level}: {shape}\")\n",
    "\n",
    "print(\"\\nOutput Shape Regresi:\")\n",
    "for level, shape in regression_shapes.items():\n",
    "    print(f\"{level}: {shape}\")\n",
    "\n",
    "print(f\"\\nTotal Anchor: {total_anchor_count}\")\n",
    "print(f\"Klasifikasi: ({total_anchor_count}, {num_classes})\")\n",
    "print(f\"Regresi: ({total_anchor_count}, 4)\")\n",
    "\n",
    "# === 10. Anchor generator ===\n",
    "input_shapes = []\n",
    "for level in range(3, 8):\n",
    "    f = features[f'p{level}']\n",
    "    stride = 2 ** level\n",
    "    input_shapes.append(ShapeSpec(\n",
    "        channels=f.shape[1],\n",
    "        height=f.shape[2],\n",
    "        width=f.shape[3],\n",
    "        stride=stride\n",
    "    ))\n",
    "\n",
    "anchor_generator = DefaultAnchorGenerator(cfg, input_shapes)\n",
    "\n",
    "# Siapkan list fitur\n",
    "feature_list = [features[f'p{l}'] for l in range(3, 8)]\n",
    "\n",
    "# Generate anchors\n",
    "anchors_over_all_levels = anchor_generator(feature_list)\n",
    "all_anchors = torch.cat([a.tensor for a in anchors_over_all_levels], dim=0)\n",
    "num_anchors = all_anchors.shape[0]\n",
    "\n",
    "# === 11. Ambil ground truth dari data_dict ===\n",
    "# Extract gt boxes dan classes\n",
    "gt_boxes_xywh = [ann[\"bbox\"] for ann in data_dict.get(\"annotations\", [])]\n",
    "gt_classes_raw = [ann[\"category_id\"] for ann in data_dict.get(\"annotations\", [])]\n",
    "\n",
    "if len(gt_boxes_xywh) == 0:\n",
    "    raise ValueError(\"Ground truth annotation kosong untuk gambar ini.\")\n",
    "\n",
    "# Konversi list ke tensor\n",
    "gt_boxes = torch.tensor(gt_boxes_xywh, dtype=torch.float32)  # (N, 4)\n",
    "gt_classes = torch.tensor(gt_classes_raw, dtype=torch.int64)  # (N,)\n",
    "\n",
    "# COCO biasanya category_id mulai dari 1, deteksi kita 0-based\n",
    "gt_classes -= 1\n",
    "\n",
    "# Konversi bbox dari [x, y, w, h] ke [x1, y1, x2, y2]\n",
    "gt_boxes[:, 2:] += gt_boxes[:, :2]\n",
    "gt_boxes = Boxes(gt_boxes)\n",
    "\n",
    "# === 12. Matching anchor dan ground truth ===\n",
    "box_matcher = Matcher(cfg.MODEL.RETINANET.IOU_THRESHOLDS,\n",
    "                      cfg.MODEL.RETINANET.IOU_LABELS,\n",
    "                      allow_low_quality_matches=True)\n",
    "\n",
    "match_quality_matrix = pairwise_iou(gt_boxes, Boxes(all_anchors))\n",
    "matched_idxs, matched_labels = box_matcher(match_quality_matrix)\n",
    "\n",
    "# === 13. Buat target klasifikasi dan regresi ===\n",
    "target_classes = torch.zeros((num_anchors, num_classes), dtype=torch.float32)\n",
    "target_deltas = torch.zeros((num_anchors, 4), dtype=torch.float32)\n",
    "\n",
    "pos_inds = torch.where(matched_labels == 1)[0]\n",
    "neg_inds = torch.where(matched_labels == 0)[0]\n",
    "\n",
    "for idx in pos_inds:\n",
    "    cls = gt_classes[matched_idxs[idx]]\n",
    "    target_classes[idx, cls] = 1.0\n",
    "\n",
    "box_transform = Box2BoxTransform(weights=(1.0, 1.0, 1.0, 1.0))\n",
    "box_transform = Box2BoxTransform(weights=(1.0, 1.0, 1.0, 1.0))\n",
    "gt_matched_boxes = gt_boxes[matched_idxs[pos_inds]]\n",
    "target_deltas[pos_inds] = box_transform.get_deltas(all_anchors[pos_inds], gt_matched_boxes.tensor)\n",
    "\n",
    "# === 14. Cetak hasil ===\n",
    "print(f\"\\nJumlah Anchor Positif: {len(pos_inds)}\")\n",
    "print(f\"Jumlah Anchor Negatif: {len(neg_inds)}\")\n",
    "\n",
    "print(\"\\nContoh Target Anchor Negatif (klasifikasi dan regresi):\")\n",
    "for i in neg_inds[:3]:\n",
    "    print(f\"[{i.item()}] Klasifikasi: {target_classes[i].numpy()}, Regresi: {target_deltas[i].numpy()}\")\n",
    "\n",
    "print(\"\\nContoh Target Anchor Positif (klasifikasi dan regresi):\")\n",
    "for i in pos_inds[:31]:\n",
    "    print(f\"[{i.item()}] Klasifikasi: {target_classes[i].numpy()}, Regresi (tx, ty, tw, th): {target_deltas[i].numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1750643506943,
     "user": {
      "displayName": "Al Qunnah",
      "userId": "06050257527007666949"
     },
     "user_tz": -420
    },
    "id": "6UJvoU5ne_fG",
    "outputId": "d9b8ea11-dcd4-4eb2-a486-d852fdcb52d2"
   },
   "outputs": [],
   "source": [
    "print(iou.shape)\n",
    "print(matched_idxs.shape)\n",
    "print(matched_labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0_GTGfOjNPp"
   },
   "source": [
    "# **Training Skenario 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3895222,
     "status": "ok",
     "timestamp": 1749877613520,
     "user": {
      "displayName": "Al Qunnah",
      "userId": "06050257527007666949"
     },
     "user_tz": -420
    },
    "id": "jCT_wN9AUOaR",
    "outputId": "8b70161b-ee64-4c6e-cc1b-e5cb06ed147f"
   },
   "outputs": [],
   "source": [
    "#trainer = DefaultTrainer(cfg)\n",
    "#trainer.resume_or_load(resume=False)\n",
    "#trainer.train()\n",
    "\n",
    "# üöÄ Train with saving best model\n",
    "from detectron2.engine.hooks import BestCheckpointer\n",
    "from detectron2.evaluation import COCOEvaluator\n",
    "from detectron2.data import build_detection_test_loader\n",
    "\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.utils.events import EventStorage\n",
    "\n",
    "class MyTrainer(DefaultTrainer):\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        return COCOEvaluator(dataset_name, cfg, False, output_folder)\n",
    "\n",
    "    def build_hooks(self):\n",
    "        hooks = super().build_hooks()\n",
    "        # Save the best checkpoint (based on bbox/AP)\n",
    "        hooks.insert(\n",
    "            -1,\n",
    "            BestCheckpointer(\n",
    "                cfg.TEST.EVAL_PERIOD,\n",
    "                checkpointer=DetectionCheckpointer(self.model, cfg.OUTPUT_DIR),\n",
    "                val_metric=\"bbox/AP\",\n",
    "                mode=\"max\",\n",
    "                file_prefix=\"best_model\"\n",
    "            )\n",
    "        )\n",
    "        return hooks\n",
    "\n",
    "trainer = MyTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpLDCdrIz2hL"
   },
   "source": [
    "# **Simpan Konfigurasi Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YbN34M8clrCF"
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(cfg.OUTPUT_DIR, \"config.yaml\"), \"w\") as f:\n",
    "    f.write(cfg.dump())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNHewVVEjUX9"
   },
   "source": [
    "# **Prediksi**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 890
    },
    "executionInfo": {
     "elapsed": 1456,
     "status": "ok",
     "timestamp": 1749877735413,
     "user": {
      "displayName": "Al Qunnah",
      "userId": "06050257527007666949"
     },
     "user_tz": -420
    },
    "id": "j4Yolk2dj3tj",
    "outputId": "45c6d206-c33b-4cec-d273-6563032d823b"
   },
   "outputs": [],
   "source": [
    "# üß† Load the trained model\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"best_model.pth\")  # Load the weights from your training\n",
    "#cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # Set threshold to 50% for detection\n",
    "\n",
    "from detectron2.engine import DefaultPredictor\n",
    "\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "# üì∏ Read the image\n",
    "#image_path = '/content/valid/00000732_005_jpg.rf.3541c4c659fec2d5d003b4fb7a380dea.jpg'\n",
    "image_path = \"/content/drive/MyDrive/210411100014_Taufiqu Reza Yoga Pratama/Dataset/test/9e90a23fb8d5ce13eb766d538afcafea_jpg.rf.0781f1855ffb9a79518d927eefc0a0cd.jpg\"\n",
    "im = cv2.imread(image_path)\n",
    "\n",
    "# üß† Run inference\n",
    "outputs = predictor(im)\n",
    "\n",
    "# Filter instances with confidence\n",
    "confidence = 0.3\n",
    "instances = outputs[\"instances\"]\n",
    "scores = instances.scores\n",
    "filtered_instances = instances[scores > confidence]\n",
    "\n",
    "# üñºÔ∏è Visualize the results\n",
    "v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(\"roboflow_train\"), scale=1)\n",
    "v = v.draw_instance_predictions(filtered_instances.to(\"cpu\"))\n",
    "\n",
    "# Show the image with filtered instances\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(v.get_image())\n",
    "plt.show()\n",
    "\n",
    "# Optionally, print out the number of detections\n",
    "print(f\"Detected {len(filtered_instances)} objects with confidence > {confidence*100}%.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_OfHDUVjYCd"
   },
   "source": [
    "# **Grafik Record Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7NJ8nzfFjsZC"
   },
   "source": [
    "**Grafik Total Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "executionInfo": {
     "elapsed": 244,
     "status": "ok",
     "timestamp": 1749877754292,
     "user": {
      "displayName": "Al Qunnah",
      "userId": "06050257527007666949"
     },
     "user_tz": -420
    },
    "id": "Ip9kvZ6fjdGZ",
    "outputId": "06c1afe7-32cc-4ad5-f42e-1cede68d3e7e"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Tentukan path ke file metrics.json\n",
    "metrics_file = '/content/drive/MyDrive/210411100014_Taufiqu Reza Yoga Pratama/Model/new/epoch_30_1e-3_SGD/metrics.json'  # Ganti dengan path file Anda\n",
    "\n",
    "# Menyaring data untuk total_loss\n",
    "iterations = []\n",
    "total_loss = []\n",
    "\n",
    "# Membaca file JSON baris per baris\n",
    "with open(metrics_file, 'r') as f:\n",
    "    for line in f:\n",
    "        # Memuat setiap baris JSON\n",
    "        entry = json.loads(line)\n",
    "\n",
    "        # Menyaring data untuk iteration dan total_loss\n",
    "        if 'iteration' in entry and 'total_loss' in entry:\n",
    "            iterations.append(entry['iteration'])\n",
    "            total_loss.append(entry['total_loss'])\n",
    "\n",
    "# Membuat grafik untuk total loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(iterations, total_loss, label=\"Total Loss\", color='tab:blue')\n",
    "\n",
    "# Menambahkan judul dan label sumbu\n",
    "plt.title('Total Loss over Iterations')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Total Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "# Menampilkan grafik\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVeZjB5JkGA_"
   },
   "source": [
    "**Grafik Average Precision**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 735
    },
    "executionInfo": {
     "elapsed": 252,
     "status": "ok",
     "timestamp": 1749877769247,
     "user": {
      "displayName": "Al Qunnah",
      "userId": "06050257527007666949"
     },
     "user_tz": -420
    },
    "id": "oLIo0Fx2kKqZ",
    "outputId": "27d712aa-8e56-42e2-e552-bfcf2e9e812c"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Tentukan path ke file metrics.json\n",
    "metrics_file = '/content/drive/MyDrive/210411100014_Taufiqu Reza Yoga Pratama/Model/new/epoch_30_1e-3_SGD/metrics.json'  # Ganti dengan path file Anda\n",
    "\n",
    "# Menyaring data untuk AP\n",
    "iterations = []\n",
    "bbox_ap = []\n",
    "bbox_ap50 = []\n",
    "bbox_ap75 = []\n",
    "bbox_ap_cardio = []\n",
    "bbox_ap_nodule = []\n",
    "bbox_ap_pneumo = []\n",
    "\n",
    "# Membaca file JSON baris per baris\n",
    "with open(metrics_file, 'r') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            entry = json.loads(line)\n",
    "\n",
    "            # Pastikan hanya memasukkan data yang mengandung metrik 'bbox/AP'\n",
    "            if 'bbox/AP' in entry:\n",
    "                iterations.append(entry['iteration'])\n",
    "                bbox_ap.append(entry['bbox/AP'])\n",
    "                bbox_ap50.append(entry.get('bbox/AP50', None))\n",
    "                bbox_ap75.append(entry.get('bbox/AP75', None))\n",
    "                bbox_ap_cardio.append(entry.get('bbox/AP-Cardiomegaly', None))\n",
    "                bbox_ap_nodule.append(entry.get('bbox/AP-Nodule-Mass', None))\n",
    "                bbox_ap_pneumo.append(entry.get('bbox/AP-Pneumothorax', None))\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing line: {e}\")\n",
    "            continue\n",
    "\n",
    "# Memastikan ada data untuk plotting\n",
    "print(f\"Total data points: {len(iterations)}\")\n",
    "\n",
    "# Jika ada data untuk plotting, buat grafik\n",
    "if iterations:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Plotting berbagai metrik AP\n",
    "    plt.plot(iterations, bbox_ap, label=\"bbox/AP\", color='tab:blue')\n",
    "    plt.plot(iterations, bbox_ap50, label=\"bbox/AP50\", color='tab:orange')\n",
    "    plt.plot(iterations, bbox_ap75, label=\"bbox/AP75\", color='tab:green')\n",
    "    plt.plot(iterations, bbox_ap_cardio, label=\"bbox/AP-Cardiomegaly\", color='tab:red')\n",
    "    plt.plot(iterations, bbox_ap_nodule, label=\"bbox/AP-Nodule-Mass\", color='tab:purple')\n",
    "    plt.plot(iterations, bbox_ap_pneumo, label=\"bbox/AP-Pneumothorax\", color='tab:brown')\n",
    "\n",
    "    # Menambahkan judul dan label sumbu\n",
    "    plt.title('Average Precision (AP) over Iterations')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Average Precision (AP)')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Menampilkan legenda\n",
    "    plt.legend()\n",
    "\n",
    "    # Menampilkan grafik\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Tidak ada data untuk plot.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VLwe32Wcl6S4"
   },
   "source": [
    "**mAP (Mean Average Precision)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1749877794436,
     "user": {
      "displayName": "Al Qunnah",
      "userId": "06050257527007666949"
     },
     "user_tz": -420
    },
    "id": "CaNRiuxnk0VI",
    "outputId": "76553408-1590-4851-fb26-21aed07d8bd4"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Menentukan path file metrics.json di Google Drive\n",
    "file_path = '/content/drive/MyDrive/210411100014_Taufiqu Reza Yoga Pratama/Model/new/epoch_30_1e-3_SGD/metrics.json'\n",
    "\n",
    "# Memuat file metrics.json\n",
    "with open(file_path, 'r') as file:\n",
    "    data = [json.loads(line) for line in file.readlines()]\n",
    "\n",
    "# Menyaring nilai AP per kelas dari data terakhir\n",
    "metrics = data[-1]  # Mengambil data terakhir yang berisi AP\n",
    "\n",
    "# Mengambil nilai AP per kelas\n",
    "ap_values = {\n",
    "    \"Cardiomegaly\": metrics.get('bbox/AP-Cardiomegaly', 0),\n",
    "    \"Nodule-Mass\": metrics.get('bbox/AP-Nodule-Mass', 0),\n",
    "    \"Pneumothorax\": metrics.get('bbox/AP-Pneumothorax', 0),\n",
    "}\n",
    "\n",
    "# Menghitung mAP (mean Average Precision)\n",
    "ap_values_list = list(ap_values.values())\n",
    "map_score = sum(ap_values_list) / len(ap_values_list)\n",
    "\n",
    "# Menampilkan hasil\n",
    "print(\"AP per kelas:\")\n",
    "for class_name, ap in ap_values.items():\n",
    "    print(f\"{class_name}: {ap:.4f}\")\n",
    "\n",
    "print(f\"\\nmAP: {map_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "executionInfo": {
     "elapsed": 207,
     "status": "ok",
     "timestamp": 1749877806640,
     "user": {
      "displayName": "Al Qunnah",
      "userId": "06050257527007666949"
     },
     "user_tz": -420
    },
    "id": "Gy55bOFflwmf",
    "outputId": "1520ffea-6c2a-4466-a0d0-165d6e279881"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Menentukan path file metrics.json (update dengan path yang sesuai)\n",
    "file_path = '/content/drive/MyDrive/210411100014_Taufiqu Reza Yoga Pratama/Model/new/epoch_30_1e-3_SGD/metrics.json'\n",
    "\n",
    "# Memuat file metrics.json\n",
    "with open(file_path, 'r') as file:\n",
    "    data = [json.loads(line) for line in file.readlines()]\n",
    "\n",
    "# Menyaring nilai AP per kelas dari data terakhir\n",
    "ap_per_iteration = []\n",
    "map_per_iteration = []\n",
    "\n",
    "# Menyaring nilai AP per kelas dan mAP dari setiap iterasi\n",
    "for metrics in data:\n",
    "    # Mengecek apakah nilai AP ada dalam data\n",
    "    if 'bbox/AP-Cardiomegaly' in metrics and 'bbox/AP-Nodule-Mass' in metrics and 'bbox/AP-Pneumothorax' in metrics:\n",
    "        ap_values = {\n",
    "            \"Cardiomegaly\": metrics.get('bbox/AP-Cardiomegaly', 0),\n",
    "            \"Nodule-Mass\": metrics.get('bbox/AP-Nodule-Mass', 0),\n",
    "            \"Pneumothorax\": metrics.get('bbox/AP-Pneumothorax', 0),\n",
    "        }\n",
    "\n",
    "        # Menghitung mAP (mean Average Precision)\n",
    "        ap_values_list = list(ap_values.values())\n",
    "        map_score = sum(ap_values_list) / len(ap_values_list)\n",
    "\n",
    "        ap_per_iteration.append(ap_values)\n",
    "        map_per_iteration.append(map_score)\n",
    "\n",
    "# Menyiapkan data untuk grafik\n",
    "iterations = range(1, len(map_per_iteration) + 1)\n",
    "\n",
    "# Plotting mAP per iterasi\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(iterations, map_per_iteration, label='mAP (mean Average Precision)', color='blue', linewidth=2)\n",
    "\n",
    "# Menambahkan grafik untuk setiap kelas\n",
    "for class_name in ap_per_iteration[0].keys():\n",
    "    class_ap_values = [ap[class_name] for ap in ap_per_iteration]\n",
    "    plt.plot(iterations, class_ap_values, label=f'AP-{class_name}', linestyle='--')\n",
    "\n",
    "# Menambahkan label dan judul\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Average Precision')\n",
    "plt.title('mAP and AP per Class over Iterations')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Menampilkan grafik\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "executionInfo": {
     "elapsed": 269,
     "status": "ok",
     "timestamp": 1749877820754,
     "user": {
      "displayName": "Al Qunnah",
      "userId": "06050257527007666949"
     },
     "user_tz": -420
    },
    "id": "-RXZyG8umbKP",
    "outputId": "4b610a07-9814-4d96-d0c3-d0a551eab6dc"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "# Path file di Google Drive (sesuaikan dengan path file Anda)\n",
    "file_path = '/content/drive/MyDrive/210411100014_Taufiqu Reza Yoga Pratama/Model/new/epoch_30_1e-3_SGD/metrics.json'\n",
    "\n",
    "# Memuat data dari file metrics.json\n",
    "with open(file_path, 'r') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "# Menyiapkan data untuk plot\n",
    "iterations = []\n",
    "loss_cls_values = []\n",
    "loss_box_reg_values = []\n",
    "\n",
    "# Looping untuk mengekstrak data\n",
    "for metrics in data:\n",
    "    # Pastikan ada key yang dibutuhkan\n",
    "    if 'iteration' in metrics:\n",
    "        iterations.append(metrics['iteration'])\n",
    "    if 'loss_cls' in metrics:\n",
    "        loss_cls_values.append(metrics['loss_cls'])\n",
    "    if 'loss_box_reg' in metrics:\n",
    "        loss_box_reg_values.append(metrics['loss_box_reg'])\n",
    "\n",
    "# Pastikan panjang data sama, dan jika tidak, sesuaikan panjangnya\n",
    "min_len = min(len(iterations), len(loss_cls_values), len(loss_box_reg_values))\n",
    "\n",
    "# Potong data agar memiliki panjang yang sama\n",
    "iterations = iterations[:min_len]\n",
    "loss_cls_values = loss_cls_values[:min_len]\n",
    "loss_box_reg_values = loss_box_reg_values[:min_len]\n",
    "\n",
    "# Membuat plot jika data ada\n",
    "if iterations and loss_cls_values and loss_box_reg_values:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot Loss Klasifikasi\n",
    "    plt.plot(iterations, loss_cls_values, label='Loss Klasifikasi', color='red', linestyle='--', linewidth=2)\n",
    "\n",
    "    # Plot Loss Regresi Batas\n",
    "    plt.plot(iterations, loss_box_reg_values, label='Loss Regresi Batas', color='green', linestyle='-', linewidth=2)\n",
    "\n",
    "    # Menambahkan label dan judul\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Klasifikasi dan Loss Regresi Batas selama Iterasi')\n",
    "\n",
    "    # Menambahkan legend\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    # Menampilkan grid\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Menampilkan grafik\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Data tidak lengkap, pastikan key 'loss_cls' dan 'loss_box_reg' ada dalam setiap entri.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFqxWH_W0Hkk"
   },
   "source": [
    "# **Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3PR-w-cxcw0Z"
   },
   "outputs": [],
   "source": [
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    box: [xmin, ymin, xmax, ymax]\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = box1\n",
    "    x1g, y1g, x2g, y2g = box2\n",
    "\n",
    "    xi1 = max(x1, x1g)\n",
    "    yi1 = max(y1, y1g)\n",
    "    xi2 = min(x2, x2g)\n",
    "    yi2 = min(y2, y2g)\n",
    "    inter_area = max(xi2 - xi1, 0) * max(yi2 - yi1, 0)\n",
    "\n",
    "    box1_area = (x2 - x1) * (y2 - y1)\n",
    "    box2_area = (x2g - x1g) * (y2g - y1g)\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "\n",
    "    if union_area == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return inter_area / union_area\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "executionInfo": {
     "elapsed": 10209,
     "status": "ok",
     "timestamp": 1749971831897,
     "user": {
      "displayName": "Al Qunnah",
      "userId": "06050257527007666949"
     },
     "user_tz": -420
    },
    "id": "ewYdvbz8cXQ3",
    "outputId": "96d2e302-26ae-4a92-aa20-ed91369da118"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# üß† Load model\n",
    "# cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"best_model.pth\")\n",
    "cfg.MODEL.WEIGHTS = \"/content/drive/MyDrive/210411100014_Taufiqu Reza Yoga Pratama/Model/new/epoch_30_1e-3_SGD/best_model.pth\"\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "# üìö Prepare dataset paths\n",
    "test_image_dir = \"/content/drive/MyDrive/210411100014_Taufiqu Reza Yoga Pratama/Dataset/dataset_new/test\"\n",
    "coco_annotation_path = \"/content/drive/MyDrive/210411100014_Taufiqu Reza Yoga Pratama/Dataset/dataset_new/test/_annotations.coco.json\"\n",
    "\n",
    "# Load COCO ground truth\n",
    "with open(coco_annotation_path) as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "# Build image_id to filename mapping\n",
    "id_to_filename = {img['id']: img['file_name'] for img in coco_data['images']}\n",
    "\n",
    "# Build annotations mapping\n",
    "annotations = coco_data['annotations']\n",
    "gt_dict = {}  # {filename: (boxes, labels)}\n",
    "\n",
    "for ann in annotations:\n",
    "    image_id = ann['image_id']\n",
    "    filename = id_to_filename[image_id]\n",
    "    bbox = ann['bbox']  # [x, y, width, height]\n",
    "    category_id = ann['category_id']\n",
    "\n",
    "    if filename not in gt_dict:\n",
    "        gt_dict[filename] = {'boxes': [], 'labels': []}\n",
    "\n",
    "    # Convert bbox format to [xmin, ymin, xmax, ymax]\n",
    "    xmin, ymin, w, h = bbox\n",
    "    xmax = xmin + w\n",
    "    ymax = ymin + h\n",
    "\n",
    "    gt_dict[filename]['boxes'].append([xmin, ymin, xmax, ymax])\n",
    "    gt_dict[filename]['labels'].append(category_id)\n",
    "\n",
    "# Build category_id to class_name mapping\n",
    "category_id_to_name = {cat['id']: cat['name'] for cat in coco_data['categories']}\n",
    "class_names = [category_id_to_name[i] for i in sorted(category_id_to_name.keys())]\n",
    "\n",
    "# üî• Start evaluating\n",
    "y_true = []\n",
    "y_pred = []\n",
    "iou_threshold = 0.5\n",
    "confidence_threshold = 0.5\n",
    "\n",
    "test_images = sorted(os.listdir(test_image_dir))\n",
    "test_images = [img for img in test_images if img.endswith('.jpg') or img.endswith('.png')]\n",
    "\n",
    "for img_file in tqdm(test_images, desc=\"Processing Test Images\"):\n",
    "    # Read image\n",
    "    img_path = os.path.join(test_image_dir, img_file)\n",
    "    img = cv2.imread(img_path)\n",
    "\n",
    "    if img_file not in gt_dict:\n",
    "        continue  # skip images without ground truth\n",
    "\n",
    "    gt_boxes = np.array(gt_dict[img_file]['boxes'])\n",
    "    gt_classes = np.array(gt_dict[img_file]['labels'])\n",
    "\n",
    "    # Predict\n",
    "    outputs = predictor(img)\n",
    "    instances = outputs[\"instances\"].to(\"cpu\")\n",
    "\n",
    "    pred_boxes = instances.pred_boxes.tensor.numpy()\n",
    "    pred_classes = instances.pred_classes.numpy()\n",
    "    pred_scores = instances.scores.numpy()\n",
    "\n",
    "    # Filter predictions by confidence\n",
    "    keep = pred_scores > confidence_threshold\n",
    "    pred_boxes = pred_boxes[keep]\n",
    "    pred_classes = pred_classes[keep]\n",
    "\n",
    "    matched_gt = set()\n",
    "\n",
    "    # Matching predicted boxes to ground truth boxes\n",
    "    for pred_box, pred_class in zip(pred_boxes, pred_classes):\n",
    "        best_iou = 0\n",
    "        best_idx = -1\n",
    "        for idx, gt_box in enumerate(gt_boxes):\n",
    "            iou = calculate_iou(pred_box, gt_box)\n",
    "            if iou > best_iou and idx not in matched_gt:\n",
    "                best_iou = iou\n",
    "                best_idx = idx\n",
    "        if best_iou >= iou_threshold and best_idx != -1:\n",
    "            y_true.append(gt_classes[best_idx])\n",
    "            y_pred.append(pred_class)\n",
    "            matched_gt.add(best_idx)\n",
    "        else:\n",
    "            # False positive\n",
    "            y_true.append(-1)  # background\n",
    "            y_pred.append(pred_class)\n",
    "\n",
    "    # False negatives\n",
    "    for idx, gt_class in enumerate(gt_classes):\n",
    "        if idx not in matched_gt:\n",
    "            y_true.append(gt_class)\n",
    "            y_pred.append(-1)\n",
    "\n",
    "# üî• Build Confusion Matrix\n",
    "all_labels = sorted(list(set([lab for lab in y_true if lab != -1] + [lab for lab in y_pred if lab != -1])))\n",
    "cm = confusion_matrix(y_true, y_pred, labels=all_labels)\n",
    "\n",
    "class_labels = [category_id_to_name[i] for i in all_labels]\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix - RetinaNet Detectron2 (COCO Format)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9591,
     "status": "ok",
     "timestamp": 1747891802790,
     "user": {
      "displayName": "Al Qunnah",
      "userId": "06050257527007666949"
     },
     "user_tz": -420
    },
    "id": "U-EJ01P3qGGv",
    "outputId": "edea73ba-d038-4466-81ed-89712ed1f5ce"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "\n",
    "# Set folder paths\n",
    "test_data_path = '/content/drive/MyDrive/210411100014_Taufiqu Reza Yoga Pratama/Dataset/dataset_new/test'  # Folder gambar test\n",
    "output_folder = '/content/drive/MyDrive/210411100014_Taufiqu Reza Yoga Pratama/output/SGD_30_1e-3_0.5'  # Folder output untuk hasil prediksi\n",
    "\n",
    "# Pastikan folder output ada\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# üß† Load the trained model\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file('/content/drive/MyDrive/210411100014_Taufiqu Reza Yoga Pratama/Model/epoch_30_1e-3_SGD/config.yaml')  # Ganti dengan path ke file konfigurasi model Anda\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"best_model.pth\")  # Path ke model .pth yang sudah dilatih\n",
    "\n",
    "# Initialize the predictor\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "# Fungsi untuk melakukan prediksi dan menyimpan gambar\n",
    "def predict_and_save_images(test_data_path, predictor, output_folder, confidence_threshold=0.2):\n",
    "    image_paths = [os.path.join(test_data_path, f) for f in os.listdir(test_data_path) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "\n",
    "    for img_path in image_paths:\n",
    "        # üì∏ Read the image\n",
    "        im = cv2.imread(img_path)\n",
    "\n",
    "        # üß† Run inference\n",
    "        outputs = predictor(im)\n",
    "\n",
    "        # Filter instances with confidence\n",
    "        instances = outputs[\"instances\"]\n",
    "        scores = instances.scores\n",
    "        filtered_instances = instances[scores > confidence_threshold]\n",
    "\n",
    "        # üñºÔ∏è Visualize the results\n",
    "        v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(\"roboflow_train\"), scale=1)\n",
    "        v = v.draw_instance_predictions(filtered_instances.to(\"cpu\"))\n",
    "\n",
    "        # Ambil nama file dan buat path untuk menyimpan gambar hasil prediksi\n",
    "        filename = os.path.basename(img_path)\n",
    "        output_path = os.path.join(output_folder, f\"pred_{filename}\")\n",
    "\n",
    "        # Simpan gambar hasil prediksi\n",
    "        cv2.imwrite(output_path, v.get_image()[:, :, ::-1])  # Convert kembali ke BGR untuk OpenCV\n",
    "        print(f\"Saved predicted image: {output_path}\")\n",
    "\n",
    "# Menjalankan prediksi dan menyimpan hasilnya\n",
    "predict_and_save_images(test_data_path, predictor, output_folder, confidence_threshold=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GRurG-eU0QaZ"
   },
   "source": [
    "# **Evaluasi Data Train, Validation, dan Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 234870,
     "status": "ok",
     "timestamp": 1748849404034,
     "user": {
      "displayName": "Al Qunnah",
      "userId": "06050257527007666949"
     },
     "user_tz": -420
    },
    "id": "En4NFoSTCIdm",
    "outputId": "919b02c5-58ff-4615-90b4-05d0a6af1f4e"
   },
   "outputs": [],
   "source": [
    "# üì¶ Install dependencies\n",
    "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
    "!pip install -q pycocotools\n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.checkpoint import DetectionCheckpointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 74,
     "status": "ok",
     "timestamp": 1749877988750,
     "user": {
      "displayName": "Al Qunnah",
      "userId": "06050257527007666949"
     },
     "user_tz": -420
    },
    "id": "vxDtjx51DYLH",
    "outputId": "ee0bad4f-4e19-4280-a95d-3e1fc3e9a5dc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "\n",
    "# Set root folder di Google Drive\n",
    "dataset_root = \"/content/drive/MyDrive/210411100014_Taufiqu Reza Yoga Pratama/Dataset/dataset_new\"\n",
    "\n",
    "# Buat path lengkap ke JSON dan folder gambar\n",
    "train_json = os.path.join(dataset_root, \"train/_annotations.coco.json\")\n",
    "val_json = os.path.join(dataset_root, \"valid/_annotations.coco.json\")\n",
    "test_json = os.path.join(dataset_root, \"test/_annotations.coco.json\")\n",
    "train_img_dir = os.path.join(dataset_root, \"train\")\n",
    "val_img_dir = os.path.join(dataset_root, \"valid\")\n",
    "test_img_dir = os.path.join(dataset_root, \"test\")\n",
    "\n",
    "# ‚úÖ Cek apakah file dan folder ada\n",
    "print(\"Train JSON exists:\", os.path.exists(train_json))\n",
    "print(\"Train Image Dir exists:\", os.path.exists(train_img_dir))\n",
    "\n",
    "# üóÇ Daftarkan dataset ke Detectron2\n",
    "register_coco_instances(\"roboflow_train_2\", {}, train_json, train_img_dir)\n",
    "register_coco_instances(\"roboflow_val_2\", {}, val_json, val_img_dir)\n",
    "register_coco_instances(\"roboflow_test_2\", {}, val_json, val_img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 153416,
     "status": "ok",
     "timestamp": 1749970827147,
     "user": {
      "displayName": "Al Qunnah",
      "userId": "06050257527007666949"
     },
     "user_tz": -420
    },
    "id": "Y5_tdhYECOBQ",
    "outputId": "288a234b-ca36-4f2e-cf9e-db9d87930ff1"
   },
   "outputs": [],
   "source": [
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "from detectron2.modeling import build_model\n",
    "\n",
    "# Load config dan model\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(\"/content/drive/MyDrive/210411100014_Taufiqu Reza Yoga Pratama/Model/new/epoch_30_1e-3_SGD/config.yaml\")\n",
    "cfg.MODEL.WEIGHTS = \"/content/drive/MyDrive/210411100014_Taufiqu Reza Yoga Pratama/Model/new/epoch_30_1e-3_SGD/best_model.pth\"\n",
    "cfg.MODEL.RETINANET.SCORE_THRESH_TEST = 0.5\n",
    "cfg.MODEL.DEVICE = 'cuda'\n",
    "cfg.DATASETS.TEST = (\"roboflow_train\",)  # ‚úÖ Kompatibilitas evaluasi\n",
    "\n",
    "# Evaluasi pada training set\n",
    "evaluator = COCOEvaluator(\"roboflow_train\", cfg, False, output_dir=\"/content/drive/MyDrive/210411100014_Taufiqu Reza Yoga Pratama/new/record_train\")\n",
    "val_loader = build_detection_test_loader(cfg, \"roboflow_train\")\n",
    "\n",
    "print(\"Evaluating on Training Set...\")\n",
    "model = build_model(cfg)\n",
    "DetectionCheckpointer(model).load(cfg.MODEL.WEIGHTS)\n",
    "results = inference_on_dataset(model, val_loader, evaluator)\n",
    "\n",
    "print(\"Evaluation Results:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10742,
     "status": "ok",
     "timestamp": 1749970633852,
     "user": {
      "displayName": "Al Qunnah",
      "userId": "06050257527007666949"
     },
     "user_tz": -420
    },
    "id": "WhbjSLU1Ex9s",
    "outputId": "8f2fcbcb-643f-4aa7-e748-d663e468d2fc"
   },
   "outputs": [],
   "source": [
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "from detectron2.modeling import build_model\n",
    "\n",
    "# Load config dan model\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(\"/content/drive/MyDrive/210411100014_Taufiqu Reza Yoga Pratama/Model/new/epoch_30_1e-3_SGD/config.yaml\")\n",
    "cfg.MODEL.WEIGHTS = \"/content/drive/MyDrive/210411100014_Taufiqu Reza Yoga Pratama/Model/new/epoch_30_1e-3_SGD/best_model.pth\"\n",
    "cfg.MODEL.RETINANET.SCORE_THRESH_TEST = 0.5\n",
    "cfg.MODEL.DEVICE = 'cuda'\n",
    "cfg.DATASETS.TEST = (\"roboflow_val\",)  # ‚úÖ Kompatibilitas evaluasi\n",
    "\n",
    "# Evaluasi pada training set\n",
    "evaluator = COCOEvaluator(\"roboflow_val\", cfg, False, output_dir=\"/content/drive/MyDrive/210411100014_Taufiqu Reza Yoga Pratama/new/record_val\")\n",
    "val_loader = build_detection_test_loader(cfg, \"roboflow_val\")\n",
    "\n",
    "print(\"Evaluating on Valid Set...\")\n",
    "model = build_model(cfg)\n",
    "DetectionCheckpointer(model).load(cfg.MODEL.WEIGHTS)\n",
    "results = inference_on_dataset(model, val_loader, evaluator)\n",
    "\n",
    "print(\"Evaluation Results:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9320,
     "status": "ok",
     "timestamp": 1749970547235,
     "user": {
      "displayName": "Al Qunnah",
      "userId": "06050257527007666949"
     },
     "user_tz": -420
    },
    "id": "hmD9IabJhpXd",
    "outputId": "896da4f9-1e60-4fd8-fc66-8f2eac8eed4c"
   },
   "outputs": [],
   "source": [
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "\n",
    "\n",
    "# Load config dan model\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(\"/content/drive/MyDrive/210411100014_Taufiqu Reza Yoga Pratama/Model/new/epoch_30_1e-3_SGD/config.yaml\")\n",
    "cfg.MODEL.WEIGHTS = \"/content/drive/MyDrive/210411100014_Taufiqu Reza Yoga Pratama/Model/new/epoch_30_1e-3_SGD/best_model.pth\"\n",
    "cfg.MODEL.RETINANET.SCORE_THRESH_TEST = 0.5\n",
    "cfg.MODEL.DEVICE = 'cuda'\n",
    "cfg.DATASETS.TEST = (\"roboflow_test\",)  # ‚úÖ Kompatibilitas evaluasi\n",
    "\n",
    "# Evaluasi pada training set\n",
    "evaluator = COCOEvaluator(\"roboflow_test\", cfg, False, output_dir=\"/content/drive/MyDrive/210411100014_Taufiqu Reza Yoga Pratama/new/record_test\")\n",
    "val_loader = build_detection_test_loader(cfg, \"roboflow_test\")\n",
    "\n",
    "print(\"Evaluating on Test Set...\")\n",
    "model = build_model(cfg)\n",
    "DetectionCheckpointer(model).load(cfg.MODEL.WEIGHTS)\n",
    "results = inference_on_dataset(model, val_loader, evaluator)\n",
    "\n",
    "print(\"Evaluation Results:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ycqMZdGBZA0g"
   },
   "outputs": [],
   "source": [
    "from detectron2.evaluation.evaluator import DatasetEvaluator\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class PRF1Evaluator(DatasetEvaluator):\n",
    "    def __init__(self, dataset_name, metadata=None, class_names=None):\n",
    "        from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "        self.metadata = MetadataCatalog.get(dataset_name)\n",
    "        self.dataset_dicts = DatasetCatalog.get(dataset_name)\n",
    "        self.class_names = class_names if class_names else self.metadata.thing_classes\n",
    "        self.gts = []\n",
    "        self.preds = []\n",
    "\n",
    "        # Buat mapping image_id ke daftar ground truth class ID\n",
    "        self.imgid_to_gt = {}\n",
    "        for data in self.dataset_dicts:\n",
    "            gt_classes = [ann[\"category_id\"] for ann in data[\"annotations\"]]\n",
    "            self.imgid_to_gt[data[\"image_id\"]] = gt_classes\n",
    "\n",
    "    def reset(self):\n",
    "        self.gts = []\n",
    "        self.preds = []\n",
    "\n",
    "    def process(self, inputs, outputs):\n",
    "        for input, output in zip(inputs, outputs):\n",
    "            img_id = input[\"image_id\"]\n",
    "            gt_classes = self.imgid_to_gt.get(img_id, [])\n",
    "            pred_classes = output[\"instances\"].pred_classes.cpu().numpy() if len(output[\"instances\"]) > 0 else []\n",
    "\n",
    "            # Samakan panjang dulu\n",
    "            if len(pred_classes) < len(gt_classes):\n",
    "                pred_classes = list(pred_classes) + [-1] * (len(gt_classes) - len(pred_classes))\n",
    "            elif len(pred_classes) > len(gt_classes):\n",
    "                pred_classes = pred_classes[:len(gt_classes)]\n",
    "\n",
    "            # Buang id 0 dan negatif setelah disamakan panjang\n",
    "            filtered = [\n",
    "                (g, p) for g, p in zip(gt_classes, pred_classes)\n",
    "                if g > 0 and p > 0\n",
    "            ]\n",
    "\n",
    "            if filtered:\n",
    "                g_filtered, p_filtered = zip(*filtered)\n",
    "                self.gts.extend(g_filtered)\n",
    "                self.preds.extend(p_filtered)\n",
    "\n",
    "    def evaluate(self):\n",
    "        print(\"\\n==== Precision, Recall, and F1-Score per Class ====\\n\")\n",
    "\n",
    "        if not self.gts or not self.preds:\n",
    "            print(\"‚ö†Ô∏è Tidak ada ground truth atau prediksi yang valid untuk dievaluasi.\")\n",
    "            return {}\n",
    "\n",
    "        # Shift kelas supaya cocok dengan sklearn (label 1‚Äì3 ‚Üí index 0‚Äì2)\n",
    "        y_true = [y - 1 for y in self.gts]\n",
    "        y_pred = [y - 1 for y in self.preds]\n",
    "\n",
    "        # Hitung classification report\n",
    "        report_dict = classification_report(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            target_names=self.class_names,\n",
    "            output_dict=True,\n",
    "            digits=3,\n",
    "            zero_division=0\n",
    "        )\n",
    "\n",
    "        # Konversi ke DataFrame\n",
    "        df = pd.DataFrame(report_dict).transpose()\n",
    "        df.index.name = 'Class'\n",
    "\n",
    "        # Ambil dan hapus akurasi dari df\n",
    "        accuracy = report_dict.get(\"accuracy\", None)\n",
    "        if \"accuracy\" in df.index:\n",
    "            df.drop(\"accuracy\", inplace=True)\n",
    "\n",
    "        # Ubah support ke integer\n",
    "        if 'support' in df.columns:\n",
    "            df['support'] = df['support'].astype(int)\n",
    "\n",
    "        # Urutkan kolom\n",
    "        df = df[['precision', 'recall', 'f1-score', 'support']]\n",
    "\n",
    "        # Cetak hasil\n",
    "        print(df.to_string(float_format=\"%.3f\"))\n",
    "        if accuracy is not None:\n",
    "            print(f\"\\nOverall Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "        # Return dict untuk logging atau visualisasi\n",
    "        return {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"classification_report\": df\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 68778,
     "status": "ok",
     "timestamp": 1749971627704,
     "user": {
      "displayName": "Al Qunnah",
      "userId": "06050257527007666949"
     },
     "user_tz": -420
    },
    "id": "WAjWAN2zZGiO",
    "outputId": "2d8e80f2-d156-4108-e367-39ece20219a1"
   },
   "outputs": [],
   "source": [
    "# Setup config\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(\"/content/drive/MyDrive/210411100014_Taufiqu Reza Yoga Pratama/Model/new/epoch_30_1e-3_SGD/config.yaml\")\n",
    "cfg.MODEL.WEIGHTS = \"/content/drive/MyDrive/210411100014_Taufiqu Reza Yoga Pratama/Model/new/epoch_30_1e-3_SGD/best_model.pth\"\n",
    "cfg.MODEL.RETINANET.SCORE_THRESH_TEST = 0.5\n",
    "cfg.MODEL.DEVICE = 'cuda'\n",
    "cfg.DATASETS.TEST = (\"roboflow_train\",)\n",
    "\n",
    "class_names = [\"Cardiomegaly\", \"Nodule/Mass\", \"Pneumothorax\"]\n",
    "\n",
    "evaluator = PRF1Evaluator(\n",
    "    \"roboflow_train\",\n",
    "    class_names=class_names\n",
    ")\n",
    "\n",
    "test_loader = build_detection_test_loader(cfg, \"roboflow_train\")\n",
    "inference_on_dataset(model, test_loader, evaluator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8732,
     "status": "ok",
     "timestamp": 1749971702672,
     "user": {
      "displayName": "Al Qunnah",
      "userId": "06050257527007666949"
     },
     "user_tz": -420
    },
    "id": "SS7tSOuXZ0vl",
    "outputId": "11b91e74-6af5-4605-e641-5952fbee9dce"
   },
   "outputs": [],
   "source": [
    "# Setup config\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(\"/content/drive/MyDrive/210411100014_Taufiqu Reza Yoga Pratama/Model/new/epoch_30_1e-3_SGD/config.yaml\")\n",
    "cfg.MODEL.WEIGHTS = \"/content/drive/MyDrive/210411100014_Taufiqu Reza Yoga Pratama/Model/new/epoch_30_1e-3_SGD/best_model.pth\"\n",
    "cfg.MODEL.RETINANET.SCORE_THRESH_TEST = 0.5\n",
    "cfg.MODEL.DEVICE = 'cuda'\n",
    "cfg.DATASETS.TEST = (\"roboflow_val\",)\n",
    "\n",
    "class_names = [\"Cardiomegaly\", \"Nodule/Mass\", \"Pneumothorax\"]\n",
    "\n",
    "evaluator = PRF1Evaluator(\n",
    "    \"roboflow_val\",\n",
    "    class_names=class_names\n",
    ")\n",
    "\n",
    "test_loader = build_detection_test_loader(cfg, \"roboflow_val\")\n",
    "inference_on_dataset(model, test_loader, evaluator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8821,
     "status": "ok",
     "timestamp": 1749971735273,
     "user": {
      "displayName": "Al Qunnah",
      "userId": "06050257527007666949"
     },
     "user_tz": -420
    },
    "id": "x3_sw_EecUwF",
    "outputId": "b6b5b91b-e9bf-4ec9-c39c-80e58f4bfaa2"
   },
   "outputs": [],
   "source": [
    "# Setup config\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(\"/content/drive/MyDrive/210411100014_Taufiqu Reza Yoga Pratama/Model/new/epoch_30_1e-3_SGD/config.yaml\")\n",
    "cfg.MODEL.WEIGHTS = \"/content/drive/MyDrive/210411100014_Taufiqu Reza Yoga Pratama/Model/new/epoch_30_1e-3_SGD/best_model.pth\"\n",
    "cfg.MODEL.RETINANET.SCORE_THRESH_TEST = 0.5\n",
    "cfg.MODEL.DEVICE = 'cuda'\n",
    "cfg.DATASETS.TEST = (\"roboflow_test\",)\n",
    "\n",
    "class_names = [\"Cardiomegaly\", \"Nodule/Mass\", \"Pneumothorax\"]\n",
    "\n",
    "evaluator = PRF1Evaluator(\n",
    "    \"roboflow_test\",\n",
    "    class_names=class_names\n",
    ")\n",
    "\n",
    "test_loader = build_detection_test_loader(cfg, \"roboflow_test\")\n",
    "inference_on_dataset(model, test_loader, evaluator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqGUmcOnXMUf"
   },
   "source": [
    "# Inference ketika tidak training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "id": "R_97Ds3cUpHK",
    "outputId": "d56c4bc5-d365-4cc2-9ef0-9bd9e1b2d5b2"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "print(\"masukkan file model_final.pth, gambar dan config.yaml\")\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 261293,
     "status": "ok",
     "timestamp": 1747212206594,
     "user": {
      "displayName": "Al Qunnah",
      "userId": "06050257527007666949"
     },
     "user_tz": -420
    },
    "id": "rn88fMocUZjz",
    "outputId": "c51b75d0-07ce-449f-c157-dc8753b4fb65"
   },
   "outputs": [],
   "source": [
    "# üì¶ Install dependencies\n",
    "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
    "!pip install -q pycocotools\n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.checkpoint import DetectionCheckpointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 983
    },
    "executionInfo": {
     "elapsed": 7396,
     "status": "ok",
     "timestamp": 1747212399138,
     "user": {
      "displayName": "Al Qunnah",
      "userId": "06050257527007666949"
     },
     "user_tz": -420
    },
    "id": "wb2mtDj2yRC6",
    "outputId": "d8f9f502-70b0-4894-ed95-3f74a66741a6"
   },
   "outputs": [],
   "source": [
    "# üí° Load the config file (you still need this to define model structure)\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(\"/content/drive/MyDrive/210411100014_Taufiqu Reza Yoga Pratama/Model/epoch_50_1e-3_SGD/config.yaml\")\n",
    "#cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # Set score threshold\n",
    "\n",
    "# üîç Set path to best model\n",
    "cfg.MODEL.WEIGHTS = \"/content/drive/MyDrive/210411100014_Taufiqu Reza Yoga Pratama/Model/epoch_50_1e-3_SGD/best_model.pth\"\n",
    "\n",
    "# üîß Build model manually (optional alternative to DefaultPredictor if you want more control)\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "# üñºÔ∏è Load and run inference on an image\n",
    "image_path = '/content/drive/MyDrive/DATA_RSUA/images/0124AE1386AN.bmp'\n",
    "im = cv2.imread(image_path)\n",
    "\n",
    "outputs = predictor(im)\n",
    "\n",
    "# Filter instances with confidence\n",
    "confidence = 0.2\n",
    "instances = outputs[\"instances\"]\n",
    "scores = instances.scores\n",
    "filtered_instances = instances[scores > confidence]\n",
    "print(filtered_instances)\n",
    "\n",
    "\n",
    "# üé® Visualize\n",
    "MetadataCatalog.get(\"roboflow_train\").thing_classes = [\"objects\", \"Cardiomegaly\", \"Nodule-Mass\", \"Pneumothorax\"]\n",
    "v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(\"roboflow_train\"), scale=1)\n",
    "v = v.draw_instance_predictions(filtered_instances.to(\"cpu\"))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(v.get_image())\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Detected {len(filtered_instances)} objects with confidence > {confidence*100}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 916
    },
    "executionInfo": {
     "elapsed": 4667,
     "status": "ok",
     "timestamp": 1747213114432,
     "user": {
      "displayName": "Al Qunnah",
      "userId": "06050257527007666949"
     },
     "user_tz": -420
    },
    "id": "u348pqz4-zle",
    "outputId": "33c30970-aa22-4a2a-f736-eb9bbe4eeacb"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "\n",
    "# üí° Load config\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(\"/content/drive/MyDrive/210411100014_Taufiqu Reza Yoga Pratama/Model/epoch_50_1e-3_SGD/config.yaml\")\n",
    "cfg.MODEL.WEIGHTS = \"/content/drive/MyDrive/210411100014_Taufiqu Reza Yoga Pratama/Model/epoch_50_1e-3_SGD/best_model.pth\"\n",
    "\n",
    "# üîç Buat predictor\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "# üñºÔ∏è Gambar yang akan diuji\n",
    "image_path = '/content/drive/MyDrive/210411100014_Taufiqu Reza Yoga Pratama/Dataset/test/a537060564b5e08c80f46362deb565e8_jpg.rf.de1b7f2a49b58f92cd288cbbb9310ecf.jpg'\n",
    "im = cv2.imread(image_path)\n",
    "\n",
    "# üîÆ Prediksi\n",
    "outputs = predictor(im)\n",
    "\n",
    "# üîé Filter prediksi berdasarkan confidence threshold\n",
    "confidence = 0.5\n",
    "instances = outputs[\"instances\"]\n",
    "scores = instances.scores\n",
    "filtered_instances = instances[scores > confidence]\n",
    "\n",
    "# üé® Visualisasi prediksi\n",
    "MetadataCatalog.get(\"roboflow_train\").thing_classes = [\"objects\", \"Cardiomegaly\", \"Nodule-Mass\", \"Pneumothorax\"]\n",
    "metadata = MetadataCatalog.get(\"roboflow_train\")\n",
    "\n",
    "v_pred = Visualizer(im[:, :, ::-1], metadata=metadata, scale=1)\n",
    "v_pred = v_pred.draw_instance_predictions(filtered_instances.to(\"cpu\"))\n",
    "img_pred = v_pred.get_image()\n",
    "\n",
    "# ‚úÖ Cari ground truth dari dataset\n",
    "dataset_dicts = DatasetCatalog.get(\"roboflow_test\")\n",
    "ground_truth = None\n",
    "for d in dataset_dicts:\n",
    "    if image_path in d[\"file_name\"]:  # atau gunakan os.path.basename\n",
    "        ground_truth = d\n",
    "        break\n",
    "\n",
    "# üé® Visualisasi ground truth\n",
    "if ground_truth:\n",
    "    v_gt = Visualizer(im[:, :, ::-1], metadata=metadata, scale=1)\n",
    "    v_gt = v_gt.draw_dataset_dict(ground_truth)\n",
    "    img_gt = v_gt.get_image()\n",
    "\n",
    "    # üîÑ Tampilkan berdampingan\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(18, 10))\n",
    "    ax[0].imshow(img_gt)\n",
    "    ax[0].set_title(\"Ground Truth\")\n",
    "    ax[0].axis(\"off\")\n",
    "\n",
    "    ax[1].imshow(img_pred)\n",
    "    ax[1].set_title(f\"Predictions (Confidence > {confidence})\")\n",
    "    ax[1].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Ground truth untuk gambar ini tidak ditemukan.\")\n",
    "    # Tampilkan hanya prediksi\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(img_pred)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Predictions (Confidence > {confidence})\")\n",
    "    plt.show()\n",
    "\n",
    "print(f\"Detected {len(filtered_instances)} objects with confidence > {confidence*100}%\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "EqGUmcOnXMUf"
   ],
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
